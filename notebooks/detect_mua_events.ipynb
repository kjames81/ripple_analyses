{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detect multi unit events\n",
    "\n",
    "    Multiunit High Synchrony Event detector. Finds times when the multiunit\n",
    "    population spiking activity is high relative to the average."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ripple_detection import multiunit_HSE_detector\n",
    "import os\n",
    "import h5py\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import hdf5storage\n",
    "from matplotlib import pyplot as plt\n",
    "import glob\n",
    "import multiprocessing\n",
    "from joblib import Parallel, delayed\n",
    "from ripple_detection.core import get_multiunit_population_firing_rate\n",
    "from scipy.signal import find_peaks,peak_prominences\n",
    "from scipy.stats import norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_position(session):\n",
    "    f = h5py.File(session,'r')\n",
    "    # load frames [ts x y a s] \n",
    "    frames = np.transpose(np.array(f['frames']))\n",
    "    return pd.DataFrame(frames,columns=['ts', 'x', 'y', 'hd', 'speed'])   \n",
    "\n",
    "def get_session_path(session):\n",
    "    f = h5py.File(session,'r')\n",
    "    return f['session_path'][()].tobytes()[::2].decode()\n",
    "\n",
    "def get_spikes(filename):\n",
    "    data = hdf5storage.loadmat(filename,variable_names=['Spikes'])\n",
    "    spike_times=data['Spikes']\n",
    "    spike_times=np.squeeze(spike_times)\n",
    "    for i in range(spike_times.shape[0]):\n",
    "        spike_times[i]=np.squeeze(spike_times[i])\n",
    "    return spike_times\n",
    "\n",
    "def bin_spikes(spike_times,dt,wdw_start,wdw_end):\n",
    "    \"\"\"\n",
    "    Function that puts spikes into bins\n",
    "    Parameters\n",
    "    ----------\n",
    "    spike_times: an array of arrays\n",
    "        an array of neurons. within each neuron's array is an array containing all the spike times of that neuron\n",
    "    dt: number (any format)\n",
    "        size of time bins\n",
    "    wdw_start: number (any format)\n",
    "        the start time for putting spikes in bins\n",
    "    wdw_end: number (any format)\n",
    "        the end time for putting spikes in bins\n",
    "    Returns\n",
    "    -------\n",
    "    neural_data: a matrix of size \"number of time bins\" x \"number of neurons\"\n",
    "        the number of spikes in each time bin for each neuron\n",
    "    \"\"\"\n",
    "    edges=np.arange(wdw_start,wdw_end,dt) #Get edges of time bins\n",
    "    num_bins=edges.shape[0]-1 #Number of bins\n",
    "    num_neurons=spike_times.shape[0] #Number of neurons\n",
    "    neural_data=np.empty([num_bins,num_neurons]) #Initialize array for binned neural data\n",
    "    #Count number of spikes in each bin for each neuron, and put in array\n",
    "    for i in range(num_neurons):\n",
    "        neural_data[:,i]=np.histogram(spike_times[i],edges)[0]\n",
    "    return neural_data\n",
    "\n",
    "def get_peak_ts(high_synchrony_event_times,firing_rate,ts):\n",
    "    peak_ts = []\n",
    "    for event in high_synchrony_event_times.itertuples():\n",
    "        idx = (ts >= event.start_time) & (ts <= event.end_time)\n",
    "        temp_ts = ts[idx]\n",
    "        peak_ts.append(temp_ts[np.argmax(firing_rate[idx])])\n",
    "    return peak_ts\n",
    "\n",
    "def fastrms(x,window=5):\n",
    "    window = np.ones(window)\n",
    "    power = x**2\n",
    "    rms = np.convolve(power,window,mode='same')\n",
    "    return  np.sqrt(rms/sum(window));\n",
    "    \n",
    "def get_place_fields(ratemap,min_peak_rate=2,min_field_width=2,max_field_width=39,percent_threshold=0.2):\n",
    "    \n",
    "    std_rates = np.std(ratemap)\n",
    "    \n",
    "    locs,properties = find_peaks(fastrms(ratemap), height=min_peak_rate, width=min_field_width)\n",
    "    pks = properties['peak_heights']\n",
    "\n",
    "    exclude = []\n",
    "    for j in range(len(locs)-1):\n",
    "        if min(ratemap[locs[j]:locs[j+1]]) > ((pks[j] + pks[j+1]) / 2) * percent_threshold:\n",
    "            if pks[j] > pks[j+1]:\n",
    "                exclude.append(j+1)\n",
    "            elif pks[j] < pks[j+1]:\n",
    "                exclude.append(j)\n",
    "       \n",
    "    if any(ratemap[locs] < std_rates*.5):\n",
    "        exclude.append(np.where(ratemap[locs] < std_rates*.5))\n",
    "    if not exclude:\n",
    "        pks = np.delete(pks, exclude)\n",
    "        locs = np.delete(locs, exclude)\n",
    "    \n",
    "    fields = []\n",
    "    for j in range(len(locs)):\n",
    "        Map_Field = (ratemap > pks[j] * percent_threshold)*1;\n",
    "        start = locs[j]\n",
    "        stop = locs[j]\n",
    "        \n",
    "        while (Map_Field[start] == 1)  & (start > 0):\n",
    "            start -= 1\n",
    "        while (Map_Field[stop] == 1)  & (stop < len(Map_Field)-1):\n",
    "            stop += 1\n",
    "\n",
    "        if ((stop - start) > min_field_width) & ((stop - start) < max_field_width):\n",
    "            com = start\n",
    "            while sum(ratemap[start:stop]) - sum(ratemap[start:com]) > sum(ratemap[start:com])/2:\n",
    "                com += 1\n",
    "            fields.append((start,stop,stop - start,pks[j],locs[j],com))\n",
    "                        \n",
    "    # add to data frames\n",
    "    fields = pd.DataFrame(fields, columns=(\"start\", \"stop\", \"width\", \"peakFR\", \"peakLoc\", \"COM\"))   \n",
    "    \n",
    "    # remove fields with the same field boundaries and keep the one with the highest peak rate\n",
    "    fields = fields.sort_values(by=['peakFR'],ascending=False)\n",
    "    fields.drop_duplicates(subset = ['start', 'stop'])\n",
    "\n",
    "    return fields\n",
    "\n",
    "def get_place_cell_idx(session):\n",
    "    \"\"\"\n",
    "    find cells to include. At least 1 field from both directions\n",
    "    \"\"\"\n",
    "    data = hdf5storage.loadmat(session,variable_names=['ratemap'])\n",
    "    include = []\n",
    "    field = 0\n",
    "    for i in range(data['ratemap'].shape[0]):\n",
    "        for d in range(2):\n",
    "            fields = get_place_fields(data['ratemap'][i,d][0])\n",
    "            if not fields.empty:\n",
    "                field += 1\n",
    "        if field > 0:\n",
    "            include.append(1)\n",
    "        else:\n",
    "            include.append(0)\n",
    "        field = 0\n",
    "    return include    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_all(session,dt=0.01):\n",
    "    \n",
    "    # get data session path from mat file\n",
    "    path = get_session_path(session)\n",
    "    \n",
    "    # load position data from .mat file\n",
    "    df = load_position(session)\n",
    "    \n",
    "    spike_times = get_spikes(session)\n",
    "    \n",
    "    # get place cells\n",
    "    include = get_place_cell_idx(session)\n",
    "    \n",
    "    multiunit = bin_spikes(spike_times[include],dt,min(df.ts),max(df.ts))\n",
    "    ts = np.arange(min(df.ts) + dt/2, max(df.ts) - dt/2, dt)\n",
    "    \n",
    "    # interp speed of the animal\n",
    "    speed = np.interp(ts,df.ts,df.speed)\n",
    "    speed[np.isnan(speed)] = 0\n",
    "    \n",
    "    # detect ripples\n",
    "    high_synchrony_event_times = multiunit_HSE_detector(ts, multiunit, speed, dt)\n",
    "    \n",
    "    # add peak time stamp\n",
    "    firing_rate = get_multiunit_population_firing_rate(multiunit, dt, 0.015)\n",
    "    peak_time = get_peak_ts(high_synchrony_event_times,firing_rate,ts)\n",
    "    high_synchrony_event_times['peak_time'] = peak_time\n",
    "    \n",
    "    return high_synchrony_event_times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_loop(session,data_path,save_path):\n",
    "    base = os.path.basename(session)\n",
    "    os.path.splitext(base)\n",
    "    save_file = save_path + os.path.splitext(base)[0] + '.pkl'\n",
    "    \n",
    "    # check if saved file exists\n",
    "    if os.path.exists(save_file):\n",
    "        return\n",
    "        \n",
    "    # detect ripples and calc some features\n",
    "    high_synchrony_event_times = run_all(session)   \n",
    "\n",
    "    # save file\n",
    "    with open(save_file, 'wb') as f:\n",
    "        pickle.dump(high_synchrony_event_times, f)\n",
    "\n",
    "\n",
    "data_path = 'F:\\\\Projects\\\\PAE_PlaceCell\\\\ProcessedData\\\\'\n",
    "save_path = \"F:\\\\Projects\\\\PAE_PlaceCell\\\\multiunit_data\\\\\"\n",
    "\n",
    "# find HPC sessions\n",
    "df_sessions = pd.read_csv('D:/ryanh/github/harvey_et_al_2020/Rdata_pae_track_cylinder_all_cells.csv')\n",
    "sessions = pd.unique(df_sessions.session)\n",
    "sessions = data_path+sessions\n",
    "\n",
    "# for session in sessions:\n",
    "#     print(session)\n",
    "#     main_loop(session,data_path,save_path)\n",
    " \n",
    "num_cores = multiprocessing.cpu_count()         \n",
    "processed_list = Parallel(n_jobs=num_cores)(delayed(main_loop)(session,data_path,save_path) for session in sessions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      start_time  end_time  peak_time                  session\n",
      "0        504.125   504.155    504.135  LEM3116_S20180715121821\n",
      "1        504.175   504.205    504.185  LEM3116_S20180715121821\n",
      "2        563.575   563.595    563.575  LEM3116_S20180715121821\n",
      "3        676.335   676.425    676.415  LEM3116_S20180715121821\n",
      "4       1403.805  1403.835   1403.805  LEM3116_S20180715121821\n",
      "...          ...       ...        ...                      ...\n",
      "7015     228.735   228.755    228.735     RH16_S20161207130000\n",
      "7016     228.865   228.885    228.865     RH16_S20161207130000\n",
      "7017     247.555   247.575    247.555     RH16_S20161207130000\n",
      "7018     276.665   276.685    276.665     RH16_S20161207130000\n",
      "7019     281.965   281.985    281.965     RH16_S20161207130000\n",
      "\n",
      "[7020 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "save_path = \"F:/Projects/PAE_PlaceCell/multiunit_data/\"\n",
    "sessions = glob.glob(save_path + '*.pkl')\n",
    "\n",
    "df=pd.DataFrame()\n",
    "for session in sessions:\n",
    "    with open(session, 'rb') as f:\n",
    "        high_synchrony_event_times = pickle.load(f)\n",
    "   \n",
    "    # add data frame of ripple features and add session id\n",
    "    base = os.path.basename(session)\n",
    "    high_synchrony_event_times['session'] = os.path.splitext(base)[0]\n",
    "    df = df.append(high_synchrony_event_times,ignore_index=True)\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data_path = 'F:\\\\Projects\\\\PAE_PlaceCell\\\\ProcessedData\\\\'\n",
    "dicts = {}\n",
    "for session in df.session:\n",
    "    f = h5py.File(data_path+session+'.mat','r')\n",
    "    ex_ep = []\n",
    "    for i in range(f['events'].shape[0]):\n",
    "        ex_ep.append(f['events'][i])\n",
    "    dicts[session] = ex_ep\n",
    "    \n",
    "ep_type = ['pedestal_1','track','pedestal_2','cylinder_1','pedestal_3','cylinder_2','pedestal_4']\n",
    "df['ep_type'] = np.ones_like(df.session)\n",
    "# session_df=pd.DataFrame()\n",
    "for session in np.unique(df.session):\n",
    "    # stack epoch times\n",
    "    b = np.hstack(dicts[session])\n",
    "    \n",
    "    # add 0 to start to indicate the start of the recording session\n",
    "    b = np.insert(b,0,0)\n",
    "    \n",
    "    # add the ts of the last ripple of the session to indicate end of session\n",
    "    b = list(b)\n",
    "    last_rip = max(df.end_time[df.session == session])\n",
    "    if b[-1] < last_rip:\n",
    "        b.append(last_rip)\n",
    "    \n",
    "    # loop through each epoch and label each ripple\n",
    "    for ep in range(len(b)-1):\n",
    "        idx = (df.session == session) & (df.peak_time >= b[ep]) & (df.peak_time <= b[ep+1])\n",
    "        df['ep_type'][idx] = ep_type[ep]\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.mkdir(save_path+'post_processed')\n",
    "df.to_csv(save_path+'post_processed/mua_df.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
